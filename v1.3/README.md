## ğŸ” ç¬¬å››ç‰ˆæ¨¡å‹ï¼ˆv1.3ï¼‰

æœ¬æ¬¡ç‚ºç¬¬ä¸‰æ¬¡ fine-tuneï¼Œæ ¹æ“š v1.2 åŸºç¤é€²è¡Œé€²ä¸€æ­¥å„ªåŒ–ï¼Œç‰¹åˆ¥å¼·åŒ–å½é“é€Ÿåº¦è¡¨ç¾èˆ‡æ–¹å‘æ§åˆ¶éˆæ•åº¦ã€‚

> âš ï¸ **ç›¸æ¯” v1.2 çš„èª¿æ•´ï¼š**
> - **å‹•ä½œç©ºé–“å¢å¼·**ï¼šå°‡ Â±15Â° è½‰å‘è§’é€Ÿåº¦ç‚º2.0 m/s æå‡è‡³ 2.5 m/sï¼Œä»¥å¼·åŒ–å½é“è¡åˆºæ•ˆç‡ã€‚
> - **reward å‡½æ•¸æ›´æ–°**ï¼šé¡å¤–æ–°å¢è½‰å‘çå‹µè¨­è¨ˆï¼Œé‡å°è½‰å‘è§’< 5 / < 15 / > 15 å°çå‹µé€²è¡Œ 1.4ã€1.0ã€0.8 èª¿æ•´ã€‚

---

### âœ… è¨“ç·´çµæœåœ–ï¼ˆç©©å®šæå‡ï¼‰

![è¨“ç·´åœ– v1.3](images/training_v1.3.png)

- ğŸŸ¢ **çå‹µåˆ†æ•¸**ï¼šæ³¢å‹•ä¸‹é™ï¼Œç©©å®šä¸Šå‡ï¼Œreward é«˜æ–¼å‰ä¸€ç‰ˆæœ¬ã€‚
- ğŸ”µ **è¨“ç·´å®Œæˆç‡**ï¼šä¸­å¾Œæ®µç©©å®šç¶­æŒåœ¨ 50â€“60%ï¼Œä»£è¡¨ç­–ç•¥é€æ¼¸æˆç†Ÿã€‚
- ğŸ”´ **è©•ä¼°å®Œæˆç‡**ï¼šå¤šæ•¸ç–Šä»£é”åˆ° 60~90%ï¼Œæ•´é«”ç©©å®šåº¦æå‡ã€‚

---

### ğŸ¥ è©•ä¼°å½±ç‰‡æˆªåœ–

![è©•ä¼°å½±ç‰‡ v1.3](images/eval_v1.3.png)

- **Best lap**ï¼š`08.617 ç§’`ï¼ˆç›®å‰æœ€ä½³ï¼‰
- **Progress**ï¼š`100%`

---

### âš™ï¸ æ¨¡å‹è¨­å®šæ‘˜è¦ï¼ˆv1.3ï¼‰

- ğŸ›£ è³½é“ï¼š`re:Invent 2018ï¼ˆé€†æ™‚é‡ï¼‰`
- ğŸ•“ è¨“ç·´æ™‚é•·ï¼š60 åˆ†é˜
- ğŸ¤– å¼·åŒ–æ¼”ç®—æ³•ï¼šPPO
- ğŸ¥ æ„Ÿæ¸¬å™¨ï¼šç›¸æ©Ÿ

#### ğŸ® å‹•ä½œç©ºé–“ï¼ˆå…± 10 çµ„ï¼‰

| åºè™Ÿ | è½‰å‘è§’ (Â°) | é€Ÿåº¦ (m/s) |
|------|------------|------------|
| 0    | -30        | 1.5        |
| 1    | -30        | 2.0        |
| 2    | -15        | 2.5        |
| 3    | -15        | 3.0        |
| 4    | 0          | 3.0        |
| 5    | 0          | 4.0        |
| 6    | 15         | 2.5        |
| 7    | 15         | 3.0        |
| 8    | 30         | 1.5        |
| 9    | 30         | 2.0        |

---

#### ğŸ”§ è¶…åƒæ•¸è¨­å®šï¼ˆv1.3ï¼‰

| åƒæ•¸åç¨±                          | æ•¸å€¼      |
|----------------------------------|-----------|
| Batch Size                       | 64        |
| Number of Epochs                | 10        |
| Learning Rate                   | 0.00005   |
| Entropy                         | 0.001     |
| Discount Factor                 | 0.99      |
| Loss Type                       | Huber     |
| Policy Update Frequency (exp)  | 20        |

---

### ğŸ§  çå‹µå‡½æ•¸ç¨‹å¼ç¢¼ï¼ˆv1.3ï¼‰

```python
def reward_function(params):
    import math

    all_wheels_on_track = params['all_wheels_on_track']
    distance_from_center = params['distance_from_center']
    track_width = params['track_width']
    speed = params['speed']
    steering = abs(params['steering_angle'])

    reward = 1e-3

    # ä¸­å¿ƒçå‹µ
    if distance_from_center <= 0.1 * track_width:
        reward = 1.0
    elif distance_from_center <= 0.25 * track_width:
        reward = 0.5
    elif distance_from_center <= 0.5 * track_width:
        reward = 0.1
    else:
        return 1e-3

    # è½‰å‘è§’æ§åˆ¶ï¼šé¼“å‹µç©©å®šæ–¹å‘ï¼Œå°è§’åº¦é«˜çå‹µ
    if steering < 5:
        reward *= 1.4
    elif steering < 15:
        reward *= 1.0
    else:
        reward *= 0.8  # éåº¦è½‰å‘æ‡²ç½°

    # é€Ÿåº¦æ§åˆ¶ï¼šåœ¨åˆé©è§’åº¦ä¸‹é¼“å‹µåŠ é€Ÿ
    if speed >= 3.0 and steering < 10:
        reward *= 1.5
    elif speed <= 2.0 and steering > 15:
        reward *= 1.2
    else:
        reward *= 0.9

    return float(reward)

