# 2025_AWS-DeepRacer_NTUT_第8組

> Reinforcement learning strategies for AWS DeepRacer — from stable baseline to sub-9 second laps on the re:Invent 2018 track.

本倉庫記錄了使用 **強化學習（Reinforcement Learning）** 技術，在 AWS DeepRacer 模擬器中針對 re:Invent 2018 賽道所進行的多階段訓練過程，最終成功達成 **單圈低於 9 秒** 的高速完賽模型。

---

## 📌 專案目標

- 建立可穩定完賽的 baseline 模型  
- 持續優化動作空間與獎勵函數  
- 提升速度同時保持穩定，挑戰圈速極限  
- 完整記錄訓練演進歷程，供後續調參或 fine-tune 使用  

---

## 🚀 專案特色

- ✅ **多版本 reward 設計（v1.0 ~ v1.3）**
- 🧠 精細轉向控制與速度獎勵聯動
- 🕹 自定義動作空間（最高達 10 組動作）
- 📊 每次訓練皆有 reward 曲線與評估圖
- 🏁 模型最終單圈最快達 **8.617 秒**

---

## 🧪 訓練版本一覽

| 版本       | 動作空間變化                                  | reward 函數邏輯                          | 超參數調整                         | 圈速最佳    | 說明重點                                      |
| -------- | --------------------------------------- | ------------------------------------ | ----------------------------- | ------- | ----------------------------------------- |
| **v1.0** | 基礎 10 組：±30° / ±15° / 0° 對應 1.0–4.0 m/s | 極簡中心線判斷，只要沒出界就給 1.0                  | 預設參數（lr 0.0003、entropy 0.01）  | 11.062s | ✅ **穩定完賽的 baseline 模型**，適合作為 fine-tune 起點 |
| **v1.1** | ±30° 與 ±15° 各增 0.5 m/s                  | 增加速度獎勵與出界懲罰，強調速度與穩定性                 | 預設參數不變     | 9.011s  | 🔧 初步進行速度優化與懲罰設計，模型可穩定加速，圈速明顯進步           |
| **v1.2** | ±15°  2.5 m/s 增加到 3.0 m/s ，測試極限速度           | 引入多層 steering-speed 聯動懲罰             | learning rate 微增至 0.00005 entropy 從 0.01 調降至 **0.001**    | 8.817s  | 🧠 精細化轉向 + 速度配對，降低亂動作機率，進一步提升彎道效率與穩定性     |
| **v1.3** | ±15°  2.0 m/s 增加到 2.5 m/s ，進一步提升最小速度    | steering 分級處理（<5°, <15°, >15°）與調整懲罰比 | 繼續使用調整後的參數 | 8.617s  | 🪄 對轉向獎懲進行精調，讓模型在彎道決策更穩定，學習速度更快           |




📄 詳細內容請見：`v1.0/README.md`、`v1.1/README.md` 等
